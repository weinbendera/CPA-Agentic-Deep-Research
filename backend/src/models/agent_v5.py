"""
DeepResearchAgent
@authors: Alec Weinbender, Michael Wood, Oliver Grudzinski
"""
import datetime
from typing import TypedDict, List, Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.tools import Tool
from tavily import TavilyClient
from langgraph.graph import StateGraph, START, END
from langchain_openai import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from pydantic import BaseModel, Field
import asyncio
import copy
import re
from datetime import datetime
import os



class SubtaskRecord(TypedDict):
    """
    SubtaskRecord captures all the state and metadata for a single subtask
    in the DeepResearchAgent's workflow. It lets each node (researcher, analyzer,
    validator, etc.) read and update a consistent structure.

    Attributes:
        retrieval_entries (List[Dict[str, Any]]):
            A chronological list of all research “hits” retrieved so far for this subtask.
            Each entry dict contains:
                - 'tool_used' (str): the name of the tool invoked (e.g. "web_search").
                - 'tool_input' (str): the exact query passed to that tool.
                - 'content' (str): the textual response returned by the tool.
            Used by the analyzer to build up a solution, and by the validator to
            check for gaps.
        solution (str):
            The most recent synthesized answer (or partial answer) for this subtask.
            Updated by the analyzer each time new findings are integrated.
        validator_feedback (str):
            Feedback from the validator node describing whether the solution is:
              • ACCEPTED — no further research needed, or
              • REVIEW_NEEDED — lists gaps and suggests additional queries.
        research_attempts (int):
            How many times the researcher node has been invoked for this subtask.
            Used by the router to enforce a maximum number of attempts.

    Example:
        >>> record: SubtaskRecord = {
        ...     "retrieval_entries": [
        ...         {
        ...             "tool_used": "web_search",
        ...             "tool_input": "IRS standard deduction 2024",
        ...             "content": "In 2024, the standard deduction is $13,850 for single filers..."
        ...         }
        ...     ],
        ...     "solution": "For 2024, single filers may claim a standard deduction of $13,850... (full report for each solution)",
                "status": "Looks Good"
        ...     "validator_feedback": "ACCEPTED: matches official IRS guidance.",
        ...     "research_attempts": 1
        ... }
    """
    retrieval_entries: List[Dict[str, Any]]
    solution: str
    validator_feedback: str
    research_attempts: int

class PlannerOutput(BaseModel):
    """
    PlannerOutput defines the JSON structure returned by the planner node.
    
    Attributes:
        subtasks (List[str]):
            A list of concise, user-specific subtasks generated by the
            planner. These represent the atomic investigation points that
            downstream nodes will process (e.g. "Identify applicable standard
            deduction for 2024", "Determine eligibility for child tax credit").
    """
    subtasks: List[str] = Field(..., description="A list of distinct, user-specific subtasks")

class ResearcherOutput(BaseModel):
    """
    ResearcherOutput defines the JSON structure returned by the researcher node.
    
    Attributes:
        tool (str):
            The name of the tool selected for gathering more information.
            Currently always 'web_search', but could expand to 'calculator',
            'tax_return_doc_search', etc.
        tool_input (str):
            The exact query or input that should be passed to the chosen tool.
            For example: 'IRS standard deduction 2024'.
        reason (str):
            A brief justification for why this tool and input were chosen—
            e.g. "Need official IRS numbers for accuracy".
    """
    tool: str = "web_search"
    # tool: str = Field(..., description="The tool to use for research")
    tool_input: str = Field(..., description="The input for the chosen tool")
    reason: str = Field(..., description="The reason for the chosen tool and input")

class AnalyzerOutput(BaseModel):
    """
    AnalyzerOutput defines the JSON structure returned by the analyzer node.
    
    Attributes:
        solution (str):
            The synthesized report or partial solution for the current subtask.
            This text integrates all retrieval entries into a coherent answer,
            formatted and tailored for a CPA audience.
    """
    solution: str = Field(..., description="The report generated from the analysis")

class ValidatorOutput(BaseModel):
    """
    ValidatorOutput defines the JSON structure returned by the validator node.
    
    Attributes:
        feedback (str):
            Free-form commentary on the current solution's completeness and
            accuracy. If REVIEW_NEEDED, feedback should include specific
            gaps or inconsistencies and suggestions for follow-up queries.
        validation_flag (str):
            One of:
              - 'ACCEPTED'      (solution is complete, no more research needed)
              - 'REVIEW_NEEDED' (gaps remain; further research required)
    """
    feedback: str = Field(..., description="Feedback on the solution")
    validation_flag: str = Field(..., description="Validation status (e.g., 'ACCEPTED', 'REVIEW_NEEDED')")

class SynthesizerOutput(BaseModel):
    """
    SynthesizerOutput defines the JSON structure returned by the synthesizer node.

    Attributes:
        report (str):
            The complete, formatted report produced by the synthesizer. This string
            contains all sections—title page, table of contents, key findings, and
            detailed subtask solutions—in Markdown (or other agreed format), ready
            for final rendering (e.g., to PDF) or direct presentation.
    """
    report: str = Field(..., description="The final full length report generated")

class AgentState(TypedDict):
    """
    The central state object passed through each node of the DeepResearchAgent's workflow.
    It holds both the original user inputs and the evolving context and results as the agent
    progresses through planning, research, analysis, validation, and synthesis.

    Attributes:
        user_question (str):
            The original question or prompt provided by the user.
        vision_payloads (List[dict]):
            Any image-based inputs accompanying the question, represented as dicts
            (e.g., with base64-encoded URLs) for tasks that require visual context.
        subtask_content (Dict[str, SubtaskRecord]):
            Maps each planned subtask description (string) to its SubtaskRecord,
            which bundles all research hits, the current solution draft, validator feedback,
            and the count of how many research attempts have been made.
        sub_tasks (List[str]):
            The ordered list of actionable subtasks generated by the planner node.
            Drives the fan-out of research/analyze/validate loops.
        current_subtask (str):
            The specific subtask currently being handled by researcher → analyzer → validator.
        validation_flag (str):
            The result of the most recent validation for the current subtask:
            either 'ACCEPTED' (complete) or 'REVIEW_NEEDED' (needs more work).
        next_node (str):
            Internal pointer naming the next node to invoke in the state graph
            (e.g., 'researcher', 'analyzer', or the END sentinel).
        chain_of_thought (str):
            Aggregated internal reasoning notes from each node invocation, useful
            for debugging and transparency into how the agent reached its conclusions.
        report (str):
            The in-memory text of the final assembled report (Table of Contents
            plus subtask solutions), before it's rendered to Markdown.
    """
    user_question: str
    vision_payloads: List[dict]
    subtask_content: Dict[str, SubtaskRecord]
    sub_tasks: List[str]
    current_subtask: str
    validation_flag: str
    next_node: str
    chain_of_thought: str
    report: str

class DeepResearchAgent:
    """
    DeepResearchAgent orchestrates a multi-stage research pipeline:
      1) PLAN: Break the user's question into discrete subtasks.
      2) RESEARCH: For each subtask, invoke external tools (e.g., web_search).
      3) ANALYZE: Summarize and integrate findings into a draft solution.
      4) VALIDATE: Check each draft for gaps or inconsistencies.
      5) SYNTHESIZE: Combine all subtask solutions into one coherent report.

    It supports parallel execution of subtasks, automatic retry up to a
    configurable limit, and final PDF/Markdown export.
    """

    def __init__(self, llm_model: str, model_key: str, tavily_api_key: str, NUM_TASKS_PER_BATCH: int = 3, MAX_RESEARCH_ATTEMPTS: int = 3):
        """
        Initialize the agent's core components and configuration.

        Args:
            llm_model (str):
                The name of the OpenAI model to use for all LLM calls
                (e.g. "gpt-4o").  
            model_key (str):
                The API key for OpenAI, used by ChatOpenAI.  
            tavily_api_key (str):
                API key for TavilyClient, used by the web_search tool.  
            NUM_TASKS_PER_BATCH (int, default=3):
                How many subtasks to request per planner invocation batch.
            MAX_RESEARCH_ATTEMPTS (int, default=1):
                Max loops through (research → analyze → validate) per subtask
                before giving up and moving on.
        """
        # === LLM setup ===
        # Core ChatOpenAI client with zero temperature for deterministic outputs
        self.llm = ChatOpenAI(model=llm_model, temperature=0, api_key=model_key)

        # === Graph setup ===
        self.subtask_graph = None

        # Wrap each pipeline stage to enforce structured JSON output
        self.structured_planner = self.llm.with_structured_output(PlannerOutput, method="function_calling")
        self.structured_researcher = self.llm.with_structured_output(ResearcherOutput, method="function_calling")
        self.structured_analyzer = self.llm.with_structured_output(AnalyzerOutput, method="function_calling")
        self.structured_validator = self.llm.with_structured_output(ValidatorOutput, method="function_calling")
        self.structured_synthesizer = self.llm.with_structured_output(SynthesizerOutput, method="function_calling")

        # === Tool clients ===
        self.tavily_client = TavilyClient(tavily_api_key)
        # Define available tools (currently only web_search)
        self.tools = [
            Tool.from_function(self.web_search, name="web_search", description=self.web_search.__doc__)
        ]

        # === Configuration params ===
        # Number of subtasks to generate per planning chunk
        self.NUM_TASKS_PER_BATCH   = NUM_TASKS_PER_BATCH
        # Maximum research loops per subtask before giving up
        self.MAX_RESEARCH_ATTEMPTS = MAX_RESEARCH_ATTEMPTS
    
    def _create_initial_state(self, user_question: str, vision_payloads: list[dict] = None) -> AgentState:
        """
        Initialize a fresh AgentState dict before any processing begins.

        Args:
            user_question (str): The original question or prompt from the user.
            vision_payloads (list[dict], optional): Any image-based inputs encoded
                as dicts (e.g. base64 URLs). Defaults to None.

        Returns:
            AgentState: A dictionary with all keys set to their empty/default values:
        """
        return {
            "user_question": user_question,
            "vision_payloads": vision_payloads,
            "subtask_content": {},
            "sub_tasks": [],
            "current_subtask": "",
            "next_node": "",
            "chain_of_thought": "",
            "validation_flag": "",
            "report": ""
        }

    def _build_subtask_graph(self) -> Any:
        """
        Construct and compile the directed state graph used for each individual subtask.

        Nodes:
            - researcher       : decides which tool/query to use next
            - analyzer         : integrates new retrievals into the current solution
            - validator        : checks the solution for completeness and gaps
            - subtask_router   : decides whether to loop back to researcher or terminate

        Edges:
            START → researcher → analyzer → validator → subtask_router
            subtask_router → conditional → researcher   (if more research needed)
            subtask_router → conditional → END          (if done)

        Returns:
            Compiled StateGraph ready for .invoke(state, config).
        """
        graph = StateGraph(AgentState)
        graph.add_node("researcher", self._researcher)
        graph.add_node("analyzer", self._analyzer)
        graph.add_node("validator", self._validator)
        graph.add_node("subtask_router", self._subtask_router)

        # Define the linear flow through the nodes
        graph.add_edge(START, "researcher")
        graph.add_edge("researcher", "analyzer")
        graph.add_edge("analyzer", "validator")
        graph.add_edge("validator", "subtask_router")

        # Loop or exit based on next_node set by subtask_router
        graph.add_conditional_edges("subtask_router", lambda state: state["next_node"],
            {   
                "researcher": "researcher", # more research needed
                "END": END # done with this subtask
            }) 
        return graph.compile()
    
    def _calculate_max_node_visits(self) -> int:
        """
        Compute a safe recursion limit for each subtask-graph invocation.

        We expect, per research loop, visits to:
            researcher → analyzer → validator → subtask_router
        That's 4 nodes per attempt. We allow MAX_RESEARCH_ATTEMPTS loops,
        plus a small BUFFER for START/END bookkeeping.

        Returns:
            int: total allowed node visits before the graph aborts.
        """
        BUFFER = 2
        recursion_limit = (1 + 1 + 1 + 1) * self.MAX_RESEARCH_ATTEMPTS + BUFFER
        print(f"Recursion Limit: {recursion_limit}")
        return recursion_limit

    async def _run_one_subtask(self, master_state: AgentState, subtask: str) -> AgentState:
        """
        Execute the research→analyze→validate loop for a single subtask in isolation.

        Args:
            master_state (AgentState): The shared state after planning, before any subtask runs.
            subtask (str): The text description of the subtask to process.

        Returns:
            AgentState: The final state for this one subtask, including its SubtaskRecord
                        and chain_of_thought log.
        """
        # 1) Deep copy the master state so parallel runs don’t interfere
        sub_state = {
            **master_state,  # copies the top-level keys/values
            "current_subtask": subtask,  # sets which subtask this branch will work on
            "subtask_content": {
                subtask: copy.deepcopy(master_state["subtask_content"][subtask])  # only copy the record this thread edits
            }
        }
        # 2) Mark which subtask this branch will work on
        sub_state["current_subtask"] = subtask
        # 3) Compute how many node visits are allowed for this invocation
        recursive_limit = self._calculate_max_node_visits()
        # 4) Invoke the compiled subtask graph in a separate thread,
        #    passing in the recursion limit to guard against infinite loops.
        #    asyncio.to_thread wraps the blocking .invoke(...) call.
        return await asyncio.to_thread(self.subtask_graph.invoke,sub_state, {"recursion_limit": recursive_limit})

    async def run(self, user_question: str, vision_payloads: list[dict] = None) -> AgentState:
        """
        Orchestrate the full research process:
          1) Plan subtasks
          2) Fan out each subtask in parallel through the subtask graph
          3) Merge results
          4) Synthesize the final report
          5) Export as PDF

        Args:
            user_question (str): The user's main prompt or question.
            vision_payloads (list[dict], optional): Any image inputs to consider.

        Returns:
            AgentState: The complete state after synthesis and PDF save.
        """
        # 1) Initialize the state and break the question into subtasks
        state = self._create_initial_state(user_question, vision_payloads)
        state = await self._planner(state)

        # 2) Compile the subtask-only graph once for reuse
        self.subtask_graph = self._build_subtask_graph()

        # 3) Launch one parallel task per subtask
        tasks = [
            self._run_one_subtask(state, sub)
            for sub in state["sub_tasks"]
        ]
        completed_states = await asyncio.gather(*tasks)

        # 4) merge all the results back into the master state
        for sub_state in completed_states:
            sub = sub_state["current_subtask"]
            record = sub_state["subtask_content"][sub]

            state["subtask_content"][sub] = record

            state["chain_of_thought"] += (
            f"\n=== Chain of Thought for '{sub}' ===\n"
            + sub_state["chain_of_thought"]
            + f"\n=== End of Chain of Thought for '{sub}' ===\n")

        # 5) Once all subtasks are done, run the synthesizer to build the final report
        state = self._synthesizer(state)
        return state

    """
    Tools used by the agent in research phase:
    """

    def web_search(self, query: str) -> str:
        """Perform a web search to retrieve new tax laws or other information you are looking for."""

        try:
            query = query.replace('"', '')

            def _run_search(restrict_domains=True):
                return self.tavily_client.search(
                    query=query,
                    search_depth="advanced",
                    max_results=3,
                    chunks_per_source=3,
                    include_answer="advanced",
                    include_domains=[
                        ".gov",  # covers IRS, ECFR, state DoRs, etc.
                        ".us",
                        "taxfoundation.org"
                    ] if restrict_domains else None
                )
            
            response = _run_search()
            hits = response.get("results", [])
            if not hits:
                print(f"\nNo hits found for query: '{query}' under primary sources i.e. irs.gov.\n")
                print(f"\nBroadening search to all domains.\n")
                # Search again with no domain restrictions if no results found
                response = _run_search(restrict_domains=False)
                hits = response.get("results", [])
            return "\n\n".join(f"{r.get('title', '')}\n{r.get('url', '')}\n{r.get('content', '')}" for r in hits)
        except Exception as e:
            return f"Web search failed due to: {e}"

    """
    Nodes for the agent's state graph:
    """

    async def _planner(self, state: AgentState) -> AgentState:
        """
        Planner Node: Break the user's question (and any provided documents/images)
        into a set of concise, actionable subtasks.

        Steps:
          1. Log the intention in chain_of_thought.
          2. Optionally load example scenarios for guidance.
          3. Construct the LLM prompt, embedding the user question.
          4. Split any large vision payloads into Base64-size-limited batches.
          5. Fire off one LLM call per batch in parallel (asyncio.to_thread).
          6. Gather and flatten all subtasks returned.
          7. Deduplicate similar subtasks by embedding similarity.
          8. Initialize state["sub_tasks"] and empty SubtaskRecord entries.
         10. Return the updated state.

        Args:
            state (AgentState): Current agent state, including 'user_question'
                                and optional 'vision_payloads'.

        Returns:
            AgentState: Updated state with:
              - sub_tasks        : List of unique subtasks
              - subtask_content  : Initialized records for each subtask
              - chain_of_thought : Planner's reasoning steps appended
        """
        # 1) Note the planner action
        state["chain_of_thought"] += f"Planner Node: deriving sub_tasks based on '{state['user_question']}'.\n"

        # 2) Try to load sample scenarios for prompt examples
        try:
            SCENARIOS_FILE = os.path.join(os.getcwd(), "data", "scenarios.md")
            scenarios = self._load_scenarios(SCENARIOS_FILE)
        except Exception as e:
            print(f"Failed to load scenarios: {e}")
            scenarios = []

        # 3) Build the LLM prompt template
        planner_prompt = f"""You are an expert CPA planning assistant.
        The user's main question is: "{state['user_question']}".
        The user has provided a PDF document with tax information and images that has been sent to you.

        Based on these inputs, break the user's question into around {self.NUM_TASKS_PER_BATCH} actionable sub-tasks or key points to investigate.
        Each sub-task should be concise, focusing on a specific aspect of the question or problem.
        Plan subtasks that would require retrieving relevant chunks from the users document or the online web.
        Only list the sub-tasks, with no extra explanation.
        Example subtasks: {[scenarios]}
        Return them as a JSON object matching the PlannerOutput schema.
        """

        # 4) Split large payloads (images) into manageable Base64‐sized batches
        MAX_B64 = 5 * 1_048_576 # 5MB
        batches = self._chunk_by_b64_chars(state["vision_payloads"], MAX_B64)

        # 5) Prepare an async helper to call the LLM in a thread for each batch
        async def call_batch(batch):
            content = [{"type":"text","text":planner_prompt}] + batch
            # Blocking .invoke wrapped in a thread to avoid blocking the event loop
            planner_response: PlannerOutput = await self.structured_planner.ainvoke(input=[{"role" : "user", "content" : content}])
            return planner_response.subtasks

        # 6) Launch all batch calls concurrently
        tasks = [call_batch(batch) for batch in batches]
        results = await asyncio.gather(*tasks)

        # 7) Flatten and dedupe the returned subtasks
        generated_subtasks = [sub for batch_subs in results for sub in batch_subs]
        state["chain_of_thought"] += f"Removing duplicate subtasks from {len(generated_subtasks)} subtasks.\n"
        print(len(generated_subtasks))
        subtasks = self._deduplicate_subtasks(generated_subtasks, threshold=0.7)
        state["sub_tasks"] = subtasks
        state["chain_of_thought"] += f"Remaining amount of subtasks: {len(subtasks)} subtasks.\n"
        print(len(subtasks))

        # 8) Initialize empty SubtaskRecord for each unique subtask
        state["subtask_content"] = {
            subtask: SubtaskRecord(retrieval_entries=[], solution="", validator_feedback="", research_attempts=0)
            for subtask in subtasks
        }
        state["chain_of_thought"] += f"Planner Node: LLM sub-task output:\n{subtasks}\n"
        print(f"Planner: The subtasks we are researching: {subtasks}")
        return state

    def _researcher(self, state: AgentState) -> AgentState:
        """
        Researcher Node: Decide which tool and query to use to fill gaps
        in the current subtask's solution, then perform that tool call.

        Steps:
          1. If there's no current_subtask, do nothing.
          2. Log entry into the chain_of_thought.
          3. Pull the existing SubtaskRecord for this subtask.
          4. Build a prompt including:
               - The subtask description
               - The current partial solution
               - Validator feedback (gaps remaining)
               - Previous retrieval entries
          5. Invoke the LLM (structured_researcher) to pick a tool_input.
          6. Default to using 'web_search' with the returned query.
          7. Execute the tool call and record its result.
          8. Append a new retrieval_entry with tool, input, reason, and content.
          9. Increment research_attempts.
         10. Return the updated state.

        Args:
            state (AgentState): Contains 'current_subtask' and
                                a SubtaskRecord in 'subtask_content'.

        Returns:
            AgentState: Updated with new retrieval entry, incremented
                        attempt count, and extended chain_of_thought.
        """
        current_subtask = state["current_subtask"]
        if not current_subtask:
            return state
        
        state["chain_of_thought"] += f"\nResearcher Node: deciding tool for '{current_subtask}'\n"

        subtask_record = state["subtask_content"].get(current_subtask, {"retrieval_entries": [], "solution": "",
                                                                        "validator_feedback": "",
                                                                        "research_attempts": 0})
        retrieval_entries = subtask_record["retrieval_entries"]
        current_solution = subtask_record["solution"]
        validator_feedback = subtask_record["validator_feedback"]

        researcher_prompt = f"""You are a researcher that is deciding what to enter as an input to conduct online
         research on the web on the given subtask and user's tax return.
        There may be previous solutions and feedback to help you with conducting what to look up to research.
        Current subtask: 
        "{current_subtask}"
        The current solution for this subtask is:
        {current_solution}
        An expert tax CPA has validated the current solution to this subtask and has provided feedback
        about gaps in the solution to research and provided insight into what to look into next:
        {validator_feedback}
        The retrieval entries found so far for this subtask are:
        {retrieval_entries}

        Provide an input that helps to fill in the gaps that the validator has provided and is relevant to the user's situation.
        The input should be a query that can be used to search the web.
        Return the query and reasoning as a JSON object matching the ResearcherOutput schema.
        """

        content = [{"type": "text", "text": researcher_prompt}]
        researcher_response: ResearcherOutput = self.structured_researcher.invoke(input=[{"role": "user", "content": content}])
        chosen_tool = "web_search"  # Default to web search
        tool_input = researcher_response.tool_input
        reason = researcher_response.reason

        state["chain_of_thought"] += (f"\nResearcher Node: Tool chosen: {chosen_tool}\n"
                                      f" Tool input: {tool_input}\n"
                                      f" Reason: {researcher_response.reason}\n")

        research = self._invoke_tool(chosen_tool, tool_input)
        entry = {"tool_used": chosen_tool, "tool_input": tool_input, "reason": reason, "research": research}

        subtask_record["retrieval_entries"].append(entry)
        state["subtask_content"][current_subtask] = subtask_record

        research_attempts = subtask_record.get("research_attempts", 0) + 1
        subtask_record["research_attempts"] = research_attempts
        state[
            "chain_of_thought"] += f"Researcher Node: Research found for use:\n{research}\n Attempt Number: {research_attempts}\n"
        print(
            f"\nResearcher: Current Subtask: {current_subtask}\nThis is research attempt: {research_attempts},\n"
            f"This is the tool input '{tool_input}'\nThis is the research found:\n{research}\n")
        return state

    def _analyzer(self, state: AgentState) -> AgentState:
        """
        Analyzer Node: Integrate new research findings into the current solution draft
        and produce an updated, well-organized report section for this subtask.

        Steps:
          1. Verify there is an active current_subtask; if not, return immediately.
          2. Retrieve the SubtaskRecord for this subtask.
          3. Build a prompt including:
               - The subtask description
               - The current partial solution
               - All accumulated retrieval entries (research hits)
               - Optional vision payloads (PDF pages or images) for context
          4. Invoke the LLM (structured_analyzer) to synthesize or update the solution.
          5. Store the returned solution text back into subtask_record["solution"].
          6. Append a note to chain_of_thought and print for debugging.
          7. Return the updated state.

        Args:
            state (AgentState): Contains 'current_subtask', previous 'solution',
                                and 'retrieval_entries' for context.

        Returns:
            AgentState: Updated with a new 'solution' in subtask_content and
                        extended chain_of_thought.
        """
        current_subtask = state["current_subtask"]
        if not current_subtask:
            return state

        subtask_record = state["subtask_content"].get(current_subtask, {"retrieval_entries": [], "solution": "",
                                                                        "validator_feedback": "",
                                                                        "research_attempts": 0})
        retrieval_entries = subtask_record["retrieval_entries"]
        current_solution = subtask_record["solution"]
        validator_feedback = subtask_record["validator_feedback"]

        if validator_feedback:
            previous_check = [{"type": "text", "text": validator_feedback}]
        else:
            previous_check = []

        analyzer_prompt = f"""You are an expert tax CPA analyzer that summarizes findings from research and develops a 
        report that creates solutions and reports details from previous findings that apply directly to the client's current situation.
        This report is to be used by other CPAs.
        You may be given the client's tax return and other useful documents to help you with this.
        You may also be given feedback about the current solution and the previous retrieval entries found.
        The current subtask is: 
        "{current_subtask}"
        The current report for this subtask is (One might not have been created yet):
        {current_solution}
        The research findings found so far for this subtask are:
        {retrieval_entries}
        The user's main question is:
        {state['user_question']}
        Ensure that the report is relevant to the user's main question, but do not state the users question in the report.
        
        Please create or update a report for this subtask that integrates the new findings and directly applies to
        the client's current situation from their document.
        
        Include a brief but comprehensive overview at the top of the report that goes over the process and main findings in the section. Do not include a conclusion in the report.

        In the report, include a section that provides the user's details/situation that are applicable to the subtask and research findings that are being used in the report.
        Ensure that all details you include are relevant to the subtask. These details are shown for review for the CPA looking at the report, so you do not need to include the name of the client.
        
        Below this, include a section that provides a summary of the research findings that are applicable to the subtask and the client's situation.
        In this same section, include your sources for the information found online by having the name of the article/page be the link.
        The sources should correspond to the research being displayed so it is easy to see where the information is from.
        
        The next two sections will depend on whether there is an inconsistency found within the client's document or not.
        
        If you have assessed that the client's document indicates a tax-saving opportunity, a compliance risk, a data inconsistency,
         or another kind of inconsistency related to the subtask and/or the user's main question, clearly state what kind of inconsistency it is.
        Include a recommendations section for the CPA looking at the report. The recommendations should come from solutions that you will create from the research findings and the client's document.
        Below the recommendations, include a savings section that highlights in what ways the client fix their inconsistencies and/or save money, and what part of client's document would relate to this.
        
        Most of the time, the client's document will be accurate on this certain subtask. If the client's document is accurate,
        the rest of the report should be based around the details related to the subtask and the client's document for review from the CPA looking at the report
        If the client's document is accurate, the report should also include why the client's document's details are correct from the research findings and the client's document.
        Include recommendations that may be helpful for the CPA and their client.
        
        The report you are making is added on to a full report so do not include who it was prepared by and the date and time it was created.
        If there is a document from the user that you believe would help for further analysis of the report, include a section
         called 'Additional Documents To Look Into' that explains what the document is and why you you would need it for further analysis.
        The report should be well-organized with sections that are easy for other CPAs to analyze and review.
        
        Return the full report as a JSON object matching the AnalyzerOutput schema.
        """

        # Add the vision payloads/user's whole file to the content
        content = ([{"type": "text", "text": analyzer_prompt}] + state["vision_payloads"] + previous_check)
        analyzer_response: AnalyzerOutput = self.structured_analyzer.invoke(input=[{"role": "user", "content": content}])
        solution = analyzer_response.solution
        subtask_record["solution"] = solution
        state["subtask_content"][current_subtask] = subtask_record

        state["chain_of_thought"] += f"\nAnalyzer Node: Solution for '{current_subtask}' is:\n{analyzer_response}\n"
        print(f"\nAnalyzer: Current Subtask: {current_subtask}\nCurrent Solution: \n{analyzer_response}\n")
        return state

    def _validator(self, state: AgentState) -> AgentState:
        """
        Validator Node: Check the current subtask's solution for completeness and accuracy.

        Steps:
          1. If there is no active current_subtask, do nothing.
          2. Retrieve the SubtaskRecord for this subtask (or defaults).
          3. Build a prompt containing:
               - The subtask description
               - The current solution draft
               - All retrieval entries (research hits)
          4. Invoke the LLM (structured_validator) to return:
               • validation_flag: 'ACCEPTED' or 'REVIEW_NEEDED'
               • feedback: explanation of gaps or confirmation of completeness
          5. Store validator_feedback in the SubtaskRecord.
          6. Update state["validation_flag"] for routing logic.
          7. Append feedback to chain_of_thought for transparency.
          8. Return the updated state.

        Args:
            state (AgentState): Holds 'current_subtask' and its existing record.

        Returns:
            AgentState: Updated with new 'validator_feedback' and 'validation_flag'.
        """
        current_subtask = state["current_subtask"]
        if not current_subtask:
            return state

        subtask_record = state["subtask_content"].get(current_subtask, {"retrieval_entries": [], "solution": "",
                                                                        "validator_feedback": "",
                                                                        "research_attempts": 0})
        retrieval_entries = subtask_record["retrieval_entries"]
        current_solution = subtask_record["solution"]

        validator_prompt = f"""You are an expert research validator that decides whether a subtask has been fully researched.
        The current subtask is: 
        '{current_subtask}'
        The current report for this subtask is:
        {current_solution}
        The retrieval entries found so far for this subtask are:
        {retrieval_entries}
        Evaluate whether the current report has any inconsistencies or gaps and applies to the user's current situation
        
        If everything is accurate and fully addressed, respond with 'ACCEPTED'. Otherwise, respond with 'REVIEW_NEEDED' as your validation_flag.
        For your feedback field, explain why you think the solution is acceptable or why it needs to be reviewed.
        If you are explaining why it needs to be reviewed, please provide a list of the gaps or inconsistencies you found
        and ways that these can be researched through online search to improve the solution.
        Return these two as a JSON object matching the ValidatorOutput schema.
        """

        content = [{"type": "text", "text": validator_prompt}]
        validator_response: ValidatorOutput = self.structured_validator.invoke(input=[{"role": "user", "content": content}])

        subtask_record["validator_feedback"] = validator_response.feedback

        state["validation_flag"] = validator_response.validation_flag
        state["chain_of_thought"] += f"\nValidator Node: Feedback for '{current_subtask}': {validator_response.feedback}\n"
        print(
            f"\nValidator: Validator's Flag: {validator_response.validation_flag}\nValidator's Feedback: {validator_response.feedback}\n")
        return state

    def _subtask_router(self, state: AgentState) -> AgentState:
        """
        Subtask Router Node: Decide whether to loop back for more research on the current subtask
        or terminate this branch and return to the master flow.

        Logic:
          1. If no current_subtask is set, do nothing.
          2. Fetch the SubtaskRecord and check how many research_attempts have occurred.
          3. If research_attempts ≥ MAX_RESEARCH_ATTEMPTS:
               - Log that we've hit the attempt cap and finalize (next_node = END).
          4. Otherwise, branch on validation_flag:
               - REVIEW_NEEDED → continue research (next_node = “researcher”)
               - ACCEPTED      → solution is complete, finalize branch (next_node = END)
               - (any other value) → error case, finalize to avoid infinite loop.
          5. Append descriptive notes to chain_of_thought for transparency.

        Args:
            state (AgentState): Contains current_subtask, its SubtaskRecord, and validation_flag.

        Returns:
            AgentState: Updated with next_node set to either "researcher" or END, plus added logs.
        """
        current_subtask = state["current_subtask"]
        if not current_subtask:
            return state
        
        subtask_record = state["subtask_content"].get(current_subtask, {"retrieval_entries": [], "solution": "",
                                                                        "validator_feedback": "",
                                                                        "research_attempts": 0})
        research_attempts = subtask_record["research_attempts"]

        if research_attempts >= self.MAX_RESEARCH_ATTEMPTS:
            # Check if the current subtask exceeds the maximum attempts.
            print("\nRouter: Reached max research attempts!\n")
            state["chain_of_thought"] += (
                f"\nSubtask Router: Reached max research attempts ({self.MAX_RESEARCH_ATTEMPTS}) "
                f"for subtask '{current_subtask}'. Finalizing this subtask.\n"
            )
            state["next_node"] = "END"
        else:
            # Condition where we have not reached the maximum attempts
            if state["validation_flag"] == "REVIEW_NEEDED":
                print(f"\nRouter: Validation flag is: {state['validation_flag']}, continuing research for: '{current_subtask}'\n")
                state["chain_of_thought"] += "Subtask Router Node: Continuing research for this subtask.\n"
                state["next_node"] = "researcher"
            elif state["validation_flag"] == "ACCEPTED":
                print(f"\nSubtask Router: Validation flag is {state['validation_flag']}, research finalized for '{current_subtask}'\n")
                state["chain_of_thought"] += f"\nSubtask Router: Research finalized for '{current_subtask}' in {research_attempts} attempts"
                state["next_node"] = "END"
            else:
                print("\n\nERROR: Validation flag not set.\n\n")
                state["chain_of_thought"] += f"Subtask Router: Validation flag is not set. Discontinuing research for subtask '{current_subtask}'.\n"
                state["next_node"] = "END"
        
        return state

    def _synthesizer(self, state: AgentState) -> AgentState:
        """
        Synthesizer Node: Generate the final, structured report from
        all subtask solutions. It builds a Table of Contents, key findings,
        and detailed solution sections, then passes the prompt to the LLM
        to render the complete report in Markdown.

        Steps:
          1. Assemble a numbered Table of Contents from state["sub_tasks"].
          2. Build each subtask's section header and solution text.
          3. Construct an LLM prompt that describes:
               - Report title page requirements (title, client name, date, agent credit).
               - Key findings table (section links, savings estimates, implementation notes).
               - Placement of TOC on its own page.
               - Detailed subtask solution pages with in-document navigation links.
          4. Invoke the structured_synthesizer to get a single `report` string.
          5. Store the resulting Markdown in state["report"].
          6. Append a chain_of_thought entry and return state.
        """

        # 1) Build Table of Contents
        toc_lines = []
        for idx, subtask in enumerate(state.get("sub_tasks", []), 1):
            toc_lines.append(f"{idx}. {subtask}")
        toc = "\n".join(toc_lines)

        # 2) Build each subtask section
        sections = []
        for idx, subtask in enumerate(state.get("sub_tasks", []), 1):
            sol = state["subtask_content"].get(subtask, {}).get("solution", "").strip()
            if not sol:
                sol = "_[no solution yet]_"
            sections.append(
                f"### {idx}. {subtask}\n\n"
                f"{sol}"
            )

        local_date = datetime.now().date().strftime("%B %d, %Y")

        synthesizer_prompt = f"""You are an expert tax CPA synthesizer that creates a final report from a Table of Contents and Solutions for each subtask.
        The report should be done in markdown only, make it so it can easily be turned into a PDF later on. 
        The report should be official and will be looked at by other CPAs and used as a reference for future tax reports.
        On the first page of the report, include the title and the date of the report. 
        Date: {local_date}
        Write prepared using Tax EvAIsion's Deep Research Agent underneath the title.
        
        On the second page, include a key findings table that includes:
            - A column labeled 'Section' that has the name of the section that corresponds to each subtask and its solution. Include a link to the section in the name.
            - A column that that labels each section as "Looks good", "Possible Savings", "Quick Reminder", and "Needs Review" depending on how much the CPA needs to look at the section
             and if there are savings that can be found in the section. Bold the text if it's labeled as "Possible Savings", "Quick Reminder", or "Needs Review."
            - A column labeled 'Savings' that contains the estimated amount of savings that can be found from each subtask and its solution. The savings should be in dollar amount.
            - A column labeled 'Dependencies' that contains a brief look at the notes/dependencies that would take place for implementing the solution.
            
        On the third page, include the Table of Contents that links to the sections in the report that correspond to each subtask.
        Here is the Table of Contents:
        {toc}
        
        On the remaining pages, include the solutions for each subtask. Here are the sections for each subtask:
        {sections}
        Include a link at each section to the Table of Contents at the bottom of each section to easily navigate to back to the Table of Contents.
        
        Do not cut out any of the previous information in the sections.
        Ensure that all sections of the report have the same markdown formatting.
        Ensure that all links in the sections have the same and correct markdown formatting.
        """

        content = [{"type": "text", "text": synthesizer_prompt}]
        synthesizer_response: SynthesizerOutput = self.structured_synthesizer.invoke(input=[{"role": "user", "content": content}])
        state["report"] = synthesizer_response.report
        print(f"\nSynthesizer: Report: \n{synthesizer_response.report}\n")
        state["chain_of_thought"] += "\nSynthesizer Node: Produced final report with TOC and ordered solutions.\n"
        return state

    """
    Helper Methods
    """

    def _deduplicate_subtasks(self, subtasks: List[str], threshold: float = 0.8) -> List[str]:
        """
        Remove similar subtasks based on cosine similarity of their embeddings.
        """
        # embed the names of each subtask
        embedder = OpenAIEmbeddings(model="text-embedding-3-small")
        embeddings = embedder.embed_documents(subtasks)
        kept_texts = []
        kept_embeds: List[List[float]] = []
        for text, emb in zip(reversed(subtasks), reversed(embeddings)):
            if not kept_embeds:
                kept_texts.append(text)
                kept_embeds.append(emb)
                continue
            sims = cosine_similarity([emb], kept_embeds)[0]
            if max(sims) < threshold:
                kept_texts.append(text)
                kept_embeds.append(emb)

        return kept_texts

    def _chunk_by_b64_chars(self, payloads: list[dict], max_chars: int):
        """
        Splits the payloads (images) into batches based on the size of the Base64 string.
        """
        batches, current, total = [], [], 0
        for p in payloads:
            # extract just the Base64 portion (after the comma)
            b64 = p["image_url"]["url"].split(",", 1)[1]
            length = len(b64)
            # if adding this page would overflow, start a new batch
            if current and total + length > max_chars:
                batches.append(current)
                current, total = [], 0
            current.append(p)
            total += length
        if current:
            batches.append(current)
        return batches
    
    def _load_scenarios(self, scenarios_file: str) -> List[str]:
        """
        Loads the scenarios from a markdown file.
        """
        with open(scenarios_file, 'r', encoding='utf-8') as file:
            content = file.read()
        pattern = r'^\d+\.\s*\*\*(.*?)\*\*\s*:\s*(.*?)(?=\n\d+\.|\Z)'
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        scenarios = []
        for title, desc in matches:
            # collapse newlines, trim whitespace
            desc = ' '.join(desc.split())
            scenarios.append(f"{title.strip()}: {desc}")
        return scenarios
    
    def _invoke_tool(self, tool_name: str, tool_input: str) -> str:
        for t in self.tools:
            if t.name == tool_name:
                result = t.invoke(tool_input)
                return self._format_result(result)
        raise ValueError(f"Tool '{tool_name}' not found.")

    ## TODO Could remove this method for another output format class for tools
    def _format_result(self, data: Any) -> str:
        if isinstance(data, str):
            return data
        if isinstance(data, list):
            lines = []
            for item in data:
                lines.append(str(item))
            return "\n".join(lines)
        return str(data)

    def _call_llm(self, prompt: str) -> str:
        """
        Call the LLM with the given prompt and return the response.
        """
        try:
            response = self.llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"Error invoking LLM: {e}"